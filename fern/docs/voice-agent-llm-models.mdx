---
title: LLM Models
subtitle: >-
  An overview of the LLM providers and models you can use with the Voice Agent
  API.
slug: docs/voice-agent-llm-models
---

Defines the LLM (*Large Language Model*) to be used with your Agent. The `provider.type` field specifies the format or protocol of the API.

For example:

- `open_ai` means the API follows OpenAI's Chat Completions format.
- This option can be used with OpenAI, Azure OpenAI, or Amazon Bedrock â€” as long as the endpoint behaves like OpenAI's Chat Completion API.


<Info>
  You can set your Voice Agent's LLM model in the [Settings Message](/docs/configure-voice-agent) See the docs for more information.
</Info>

## Supported LLM providers

You can query the following endpoint to check the supported models for each provider:

```cURL
curl https://agent.deepgram.com/v1/agent/settings/think/models
```

### Example Payload

```json
{
  "models": [
    {
      "id": "gpt-4.1-mini",
      "name": "GPT-4.1 mini",
      "provider": "open_ai"
    },
    {
      "id": "gpt-4.1-nano",
      "name": "GPT-4.1 nano",
      "provider": "open_ai"
    },
    {
      "id": "gpt-4o-mini",
      "name": "GPT-4o mini",
      "provider": "open_ai"
    },
    {
      "id": "claude-3-haiku-20240307",
      "name": "Claude Haiku 3",
      "provider": "anthropic"
    },
    {
      "id": "claude-3-5-haiku-latest",
      "name": "Claude Haiku 3.5",
      "provider": "anthropic"
    }
  ]
}

```

<Info>
   If you don't specify `agent.think.provider.type` the Voice Agent will use Deepgram's default managed LLMs. For managed LLMs, supported model names are predefined in our configuration.
</Info>


| Parameter                      | `open_ai` | `anthropic` | `x_ai`   |
| ------------------------------ | --------- | ----------- | -------- |
| `agent.think.provider.type`    | `open_ai` | `anthropic` | `x_ai`   |
| `agent.think.endpoint`         | optional  | optional    | required |


The `agent.think.endpoint` is optional or required based on the provider type:

* For `open_ai` and `anthropic`, the `endpoint` field is optional because Deepgram provides managed LLMs for these providers.
* For all other provider types, `endpoint` is required because Deepgram does not manage those LLMs.
* If an `endpoint` is provided the `url` is required but `headers` are optional.

## Supported LLM models

### OpenAI

| Provider                      | Model      | Availability |
| ------------------------------ | --------- |--------|
| `open_ai`    | `gpt-4.1` | `Coming Soon!` |
| `open_ai`    | `gpt-4.1-mini` | `GA` |
| `open_ai`    | `gpt-4.1-nano` | `GA` |
| `open_ai`    | `gpt-4o` | `Coming Soon!` |
| `open_ai`    | `gpt-4o-mini` | `GA` |

### Anthropic

| Provider                      | Model      | Availability |
| ------------------------------ | --------- |--------|
| `anthropic`    | `claude-3-haiku-20240307` | `GA` |
| `anthropic`    | `claude-3-5-haiku-latest` | `GA` |
| `anthropic`    | `claude-4-sonnet` | `Coming Soon!` |

### Google

| Provider                      | Model      | Availability |
| ------------------------------ | --------- |--------|
| `google`    | `gemini-2-5-flash-preview` | `Coming Soon!` |
| `google`    | `gemini-2-0-flash` | `Coming Soon!` |
| `google`    | `gemini-2-0-flash-lite` | `Coming Soon!` |

## Example Payload

<CodeGroup>
  ```json JSON
  // ... other settings ...
   "think": {
        "provider": {
          "type": "open_ai",
          "model": "gpt-4o-mini",
          "temperature": 0.7
        },
        "endpoint": { // Optional if LLM provider is open_ai or anthropic. Required for non-Deepgram LLM providers like x_ai
          "url": "https://api.example.com/llm", // Required if endpoint is provided
          "headers": { // Optional if an endpoint is provided
            "authorization": "Bearer {{token}}"
          }
        },
  // ... other settings ...
  ```
</CodeGroup>

## Passing a custom (BYO) LLM through a Cloud Provider

<Info>
  For Bring Your Own (BYO) LLMs, any model string provided is accepted without restriction.
</Info>

You can use a custom LLM hosted by a 3rd party Cloud Provider by setting the `provider.type` to one of the supported provider values and setting the `endpoint.url` and `endpoint.headers` fields to the correct values for your Cloud Provider.

<CodeGroup>
  ```json JSON
  {
    // ... other settings ...
  "think": {
        "provider": {
          "type": "open_ai",
          "model": "gpt-4",
          "temperature": 0.7
        },
        "endpoint": { // Required for a custom LLM
          "url": "https://cloud.provider.com/llm", // Required for a custom LLM
          "headers": { // Optional for a custom LLM
            "authorization": "Bearer {{token}}"
          }
        },
    // ... other settings ...
  }
  ```</CodeGroup>

