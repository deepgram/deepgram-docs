---
title: LLM Models
subtitle: >-
  An overview of the LLM providers and models you can use with the Voice Agent
  API.
slug: docs/voice-agent-llm-models
---

Defines the LLM (*Large Language Model*) to be used with your Agent. The `provider.type` field specifies the format or protocol of the API.

For example:

- `open_ai` means the API follows OpenAI’s Chat Completions format.
- This option can be used with OpenAI, Azure OpenAI, or Amazon Bedrock — as long as the endpoint behaves like OpenAI’s Chat Completion API.


<Info>
  You can set your Voice Agent's LLM model in the [Settings Message](/docs/configure-voice-agent) See the docs for more information.
</Info>

## Supported LLM providers

<Info>
   If you don't specify `agent.think.provider.type` the Voice Agent will use Deepgram's default LLM.
</Info>


| Parameter                      | `open_ai` | `anthropic` | `x_ai`   |
| ------------------------------ | --------- | ----------- | -------- |
| `agent.think.provider.type`    | `open_ai` | `anthropic` | `x_ai`   |
| `agent.think.endpoint`         | optional  | optional    | required |


The `agent.think.endpoint` is optional or required based on the provider type:

* For `open_ai` and `anthropic`, the `endpoint` field is optional because Deepgram provides managed LLMs for these providers.
* For all other provider types, `endpoint` is required because Deepgram does not manage those LLMs.
* If an `endpoint` is provided the `url` is required but `headers` are optional.

## Supported LLM models

### OpenAI

| Provider                      | Model      |
| ------------------------------ | --------- |
| `open_ai`    | `gpt-4o-mini` |
| `open_ai`    | `gpt-3-5-turbo` |
| `open_ai`    | `gpt-4o` |
| `open_ai`    | `gpt-4o-search-preview` |

### Anthropic

| Provider                      | Model      |
| ------------------------------ | --------- |
| `anthropic`    | `claude-3-haiku-20240307` |


## Example Payload

<CodeGroup>
  ```json JSON
  // ... other settings ...
   "think": {
        "provider": {
          "type": "open_ai",
          "model": "gpt-4o-mini",
          "temperature": 0.7
        },
        "endpoint": { // Optional if LLM provider is open_ai or anthropic. Required for non-Deepgram LLM providers like x_ai
          "url": "https://api.example.com/llm", // Required if endpoint is provided
          "headers": { // Optional if an endpoint is provided
            "authorization": "Bearer {{token}}"
          }
        },
  // ... other settings ...
  ```
</CodeGroup>

## Passing a custom LLM through a Cloud Provider

You can use a custom LLM hosted by a 3rd party Cloud Provider by setting the `provider.type` to one of the supported provider values and setting the `endpoint.url` and `endpoint.headers` fields to the correct values for your Cloud Provider.

<CodeGroup>
  ```json JSON
  {
    // ... other settings ...
  "think": {
        "provider": {
          "type": "open_ai",
          "model": "gpt-4",
          "temperature": 0.7
        },
        "endpoint": { // Required for a custom LLM
          "url": "https://cloud.provider.com/llm", // Required for a custom LLM
          "headers": { // Optional for a custom LLM
            "authorization": "Bearer {{token}}"
          }
        },
    // ... other settings ...
  }
  ```
</CodeGroup>
